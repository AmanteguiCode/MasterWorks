---
title: "Time Series Analysis and Forecasting - Exercise Set 2"
author: "Borja Ruiz"
date: "11 marzo 2018"
output:
  pdf_document:
    fig_caption: yes
    fig_height: 3.5
    fig_width: 4
    latex_engine: xelatex
    number_sections: yes
    toc: yes
  html_document:
    df_print: paged
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage
#Exercise 1

Data set books contains the daily sales of paperback and hardcover books at the same store. The task is to forecast the next four days' sales for hardcover books (data set books).

**A)** Plot the series and discuss the main features of the data.

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
library(fpp)
library(fma)
data("books")
par(mfrow = c(1, 1))
plot(books[,1], xlab = "Day", ylab = "Hardcover",main=paste("Borja Ruiz, Timestamp:",Sys.time()))

```

We can appreciate a positive trend a some possible cyclic behaviour in the data.

**B)** Use simple exponential smoothing with the ses function (setting initial = "simple") and explore different values of $\alpha$ for the paperback series. Record the within-sample SSE for the one-step forecasts. 
Plot SSE against $\alpha$ and find which value of $\alpha$ works best. What is the effect of $\alpha$ on the forecasts?


```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
books1 <- window(books[,1], start = 1, end = 24)
bfit1 <- ses(books1, alpha = 0.2, initial = "simple", h = 3)
bfit2 <- ses(books1, alpha = 0.4, initial = "simple", h = 3)
bfit3 <- ses(books1, alpha=0.6, initial = "simple",h = 3)
bfit4 <- ses(books1, alpha=0.8, initial = "simple",h = 3)
plot(bfit1, PI=FALSE, ylab="",
     xlab="Day", main=paste("Borja Ruiz, Timestamp:",Sys.time()), fcol=1, type="o")
lines(fitted(bfit2), col="blue", type="o")
lines(fitted(bfit3), col="red", type="o")
lines(fitted(bfit4), col="green", type="o")
lines(bfit1$mean, col="blue", type="o")
lines(bfit2$mean, col="red", type="o")
lines(bfit3$mean, col="green", type="o")
lines(bfit4$mean, col="pink", type="o")

        
```
```{r}
books1 <- window(books[,1], start = 25, end = 30)
a1<-accuracy(bfit1, books1);a1
a2<-accuracy(bfit2, books1);a2
a3<-accuracy(bfit3, books1);a3
a4<-accuracy(bfit4, books1);a4
```

$\alpha = 0.8$ seems to be the one to improve the accuracy.

It seems the higher the alpha the smoother the time series seems, therefore high and low peaks disappear progressively, and differente features in the data are appreciated.

**C)** Now let ses select the optimal value of $\alpha$. Use this value to generate forecasts for the next four days. Compare your results with (b).

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
books1 <- window(books[,1], start = 1, end = 26)
fit1 <- ses(books1, initial = "simple", h = 4)
fit2 <- holt(books1, initial = "simple", h = 4)
par(mfrow = c(3,2))
plot(fit1, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
plot(fit2, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
plot(bfit1, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
plot(bfit2, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
plot(bfit3, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
plot(bfit4, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
```

Since we have used Holt's linear trend method we are able to perceive that our predictions now have adquired a slight negative trend. We are also able to make predictions near in time more precisely, though our confidence intervals grow bigger the more $h$ increases.

Relative to the ses choosing the value of $\alpha$, we appreciate that the forecasting produced would seem to have approximately a $\alpha = 0.3$.

#Exercise 2 

Use the monthly Australian short-term overseas visitors data, May 1985-April 2005. (Data set: visitors)

**A)** Make a time plot of your data and describe the main features of the series.

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
data("visitors")
par(mfrow = c(1, 1))
plot(visitors, xlab = "Day", ylab = "Visitors",main=paste("Borja Ruiz, Timestamp:",Sys.time()))
```
**B)** Forecast the next two years using Holt-Winters' multiplicative method.

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
visitors1<-window(visitors,end=2003)
fit6 <- hw(visitors1, h=24, seasonal = "multiplicative")
par(mfrow = c(1,1))
plot(fit6, ylab="Visitors", main=paste("Borja Ruiz, Timestamp:",Sys.time()), flwd=1, PI=FALSE)

```
**C)** Why is multiplicative seasonality necessary here?

Multiplicative method is necessary since we observe a growth in the seasonal variation in the data along time, and this is best addressed with multiplicative methods.


**D)** Experiment with making the trend exponential and/or damped.

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}

fit7 <- holt(visitors1,h=24)
fit8 <- holt(visitors1,h=24, exponential = TRUE)
fit9 <- holt(visitors1,h=24, damped = TRUE)
fit10 <- holt(visitors1,h=24, exponential = TRUE, damped = TRUE)

par(mfrow=c(2,2))
accuracy(fit8, window(visitors, start = 2003))
accuracy(fit9, window(visitors, start = 2003))
accuracy(fit10, window(visitors, start = 2003))

par(mfrow=c(2,2))
plot(fit7$model$state, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
plot(fit8$model$state, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
plot(fit9$model$state, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
plot(fit10$model$state, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
```
```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}

plot(fit7, xlab = "Day", ylab = "Visitors",main=paste("Borja Ruiz, Timestamp:",Sys.time()))
lines(fit8$mean, col = "red")
lines(fit9$mean, col = "blue")
lines(fit10$mean, col = "pink")
```

We are able to appreciate how the slope of the trends gets modified.

**E)** Now fit each of the following models to the same data: 
1. an ETS model
2. an additive ETS model applied to a Box-Cox transformed series
3. an STL decomposition applied to the Box-Cox transformed data followed by an ETS model applied to the seasonally adjusted (transformed) data.     
Plot all the forecasts together.

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
par(mfrow=c(3,1))

vdata <- window(visitors, end = 2003)
fit1 <- ets(vdata)
plot(forecast(fit1), main=paste("Borja Ruiz, Timestamp:",Sys.time()))


vdatadiff <- window(diff(log(visitors)), end = 2003)
fit2<- ets(vdatadiff)
plot(forecast(fit2), col="red", main=paste("Borja Ruiz, Timestamp:",Sys.time()))

fit3 <- stl(vdatadiff, t.window = 50, s.window="periodic", robust=TRUE)
plot(forecast(fit3), main=paste("Borja Ruiz, Timestamp:",Sys.time()))

```

#Exercise 3

Consider the quarterly number of international tourists to Australia for the period 1999-2010. (Data set austourists.)

**A)** Describe the time plot.

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
data("austourists")
par(mfrow = c(1, 1))
plot(austourists, xlab = "Quarter", ylab = "Tourists", main=paste("Borja Ruiz, Timestamp:",Sys.time()))
```

The time series shows a seasonality pattern within a positive trend.

**B)** What can you learn from the ACF graph?

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
Acf(austourists, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
```

Since we can observe high autocorrelations in seasonality and in the trend we assume that the dataset is not stationary. We can see that we can choose 4 possible values of $m$ for the AR model, 4, 8, 12 and 16.

**C)** What can you learn from the PACF graph?

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
Pacf(austourists, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
```

After eliminating the partial correlations we can infer that the optimum lag value is $m=4$ in a AR(1) model since we have $p=1$.

**D)** Produce plots of the seasonally differenced data $(1-B^4)Y_t$ . 

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
par(mfrow = c(1,2))
plot(diff(austourists, 4), main=paste("Borja Ruiz, Timestamp:",Sys.time()))

```

What model do these graphs suggest?

We confirm that $m=4$ is a good value since the graph seems a lot more stationary and $D=1$ could be a possible value.

**E)** Does auto.arima give the same model that you chose? If not, which model do you think is better?

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
auto.arima(austourists)
```

Auto arima chose the same model as me.

#Exercise 4 

Consider the total net generation of electricity (in billion kilowatt hours) by the U.S. electric industry (monthly for the period 1985-1996). (Data set usmelec.) In general there are two peaks per year: in mid-summer and mid-winter.

**A)** Examine the 12-month moving average of this series to see what kind of trend is involved.   

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
data("usmelec")
par(mfrow = c(1, 1))
plot(usmelec, xlab = "Months", ylab = "Electricity",main=paste("Borja Ruiz, Timestamp:",Sys.time()))
```
We appreciate a positive trend and a seasonal variation.


**B)** Do the data need transforming? If not, find an appropriate differencing which yields stationary data.

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
par(mfrow=c(2,2))
plot(log(usmelec), main=paste("Borja Ruiz, Timestamp:",Sys.time()))
plot(diff(usmelec,6), main=paste("Borja Ruiz, Timestamp:",Sys.time()))
plot(diff(log(usmelec),6), main=paste("Borja Ruiz, Timestamp:",Sys.time()))
plot(diff(diff(log(usmelec),6),1), main=paste("Borja Ruiz, Timestamp:",Sys.time()))
```

To transform the data we first demonstrate that using the log function minimizes the seasonal variance. Furthermore applying the difference between intervals will eliminate the seasonality. 6 is a good value for the difference stimation since we appreciate peaks every 6 months in the original data.

We finally applied both transformations to obtain a stationary structure from our original data. We can also apply another difference transformation to obtain white noise.

**C)** Identify a couple of ARIMA models that might be useful in describing the time series. Which of your models is the best according to their AICc values?


```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
acf(usmelec, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
pacf(usmelec, main=paste("Borja Ruiz, Timestamp:",Sys.time()))

Arima(usmelec, order = c(2,2,0), seasonal = c(2,2,0))
Arima(usmelec, order = c(1,1,0), seasonal = c(1,1,0))
Arima(usmelec, order = c(2,1,0), seasonal = c(2,1,0))
```

Since we can observe in the ACF and PACF graphs two possible values for $m$, we decide to play with values of $p={1,2}$. Finally our best arima model will be A(2,1,0)(2,1,0). We now try this value with our transformed data:

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
fitarima <- Arima(diff(log(usmelec),6), order = c(2,1,0), seasonal = c(2,1,0))
```

**D)** Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise? If not, try to find another ARIMA model which fits better.

```{r, echo=TRUE,fig.height = 5, fig.width = 14, fig.align='center'}
res <- residuals(fitarima)
tsdisplay(res, main=paste("Borja Ruiz, Timestamp:",Sys.time()))
```